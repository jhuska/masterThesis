% fithesis2 with modifications used, please use local fithesis.cls file, not system-wide installed.
\documentclass[11pt,oneside,final]{fithesis2}
% \documentclass[oneside,final]{fithesis2}
% \usepackage[resetfonts]{cmap}
\usepackage{lmodern}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{afterpage}
\usepackage{calc}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{float}
\restylefloat{figure}

\usepackage{listings}
\usepackage{fixltx2e}

\def\R{\mbox{\sffamily\bfseries R}}

\DeclareGraphicsExtensions{.pdf,.png,.jpg,.gif}

\thesislang{en}
\thesistitle{Visual testing something catchy}
\thesissubtitle{Diploma thesis}
\thesisstudent{Juraj Húska}
\thesiswoman{false}
\thesisfaculty{fi}
\thesisyear{2015}
\thesisadvisor{Mgr.\,Marek Grác,\,Ph.D.}

\usepackage{url}
\usepackage[numbers]{natbib}
\bibliographystyle{unsrtnat}
%\bibliographystyle{plain}

\usepackage{fancyhdr}
\pagestyle{plain}

% multi-row
%\usepackage{multirow}
\usepackage{color, colortbl}
\usepackage{enumerate}

\definecolor{Gray}{gray}{0.85}
\newcommand{\clg}{\cellcolor{Gray}}
\newcommand{\eal}{\emph{et~al.}}

% \fancyhead[LE,RO]{\slshape \rightmark}
% \fancyhead[LO,RE]{\slshape \leftmark}
% \fancyfoot[C]{\thepage}


\hyphenation{how-to}

\begin{document}


\newenvironment{atribut_description}
{\begin{description}
  \renewcommand{\makelabel}[1]{\texttt{\hspace{6pt}##1 $-$}}%
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}
{\end{description}}
\renewcommand{\tiny}{\fontsize{7.7}{9.7}\selectfont}

\FrontMatter
\ThesisTitlePage

\begin{ThesisDeclaration}
\DeclarationText
\AdvisorName
\end{ThesisDeclaration}

\begin{ThesisThanks}
Some people helped me a lot and some not at all. Nevertheless, I would like to thank all.
\end{ThesisThanks}

\begin{ThesisAbstract}
This thesis is very important!
\end{ThesisAbstract}
 
\begin{ThesisKeyWords}
key word1, and so on
\end{ThesisKeyWords}
\MainMatter



\renewcommand{\contentsname}{Table of contents}

\tableofcontents

\chapter{Introduction}    
There is a big demand for this thesis.
Need and cost of manual testing, space for improvement.
    
\chapter{Visual testing of software}    
    Testing of software in general is any activity aimed at evaluating an attribute or capability of a program and determining that it meets its required results [1]. 
    It can be done either manually by actual using of an application, or automatically by executing testing scripts.
    
    If the application under test has also a graphical user interface (GUI), then one has to verify whether it is not broken. 
    Visual testing of an application is an effort to find out its non-functional errors, which expose themselves by changing a graphical state of the application under test.
    
    Typical example can be a web application, which GUI is programmed usually with combination of HyperText Markup Language (HTML) and Cascading Style Sheets (CSS). 
    HTML is often used to define a content of the web application (such as page contains table, pictures, etc.), while CSS defines a structure and appearance of the 
    web application (such as color of the font, absolute positioning of web page elements, and so on).
    
    The resulting web application is a set of rules (CSS and HTML) applied to a static content (e.g. pictures, videos, text). The combination of rules is crucial, and a minor change
    can completely change the visual state of the web application. Such changes are very difficult, sometimes even not possible to find out by functional tests of the application. 
    It is because functional tests verify a desired functionality of the web application, and do not consider web page characteristics such as red color of heading, 
    space between two paragraphs, and similar.
    
    That is why a visual testing has to take a place. Again, it is done either manually, when a tester by working with an application, is going through all of its use cases, and verifies, that
    the application has not broken visually. Or automatically, by executing scripts which assert a visual state of an application.
    
    In this thesis we are going to focus on the visual testing of web applications only. As we mentioned above, the way how web page looks like is mainly determined by CSS script.
    There are two ways of automated testing used:
    \begin{enumerate}
      \item asserting the CSS script
      \item or comparing screen captures (also known as screenshots) of new and older versions of the application.
    \end{enumerate}
     
  \section{Visual testing in release testing process}
  \label{sec:visual-testing-in-release-process}
  Nowadays software is often released for a general availability in repetitive cycles, which are defined according to a particular software development process. 
  Such as Waterfall [2], or Scrum [3].
  
  Testing of software has an immense role in this release process. While automated tests are often executed continuously, as they are quicker to run than manual tests, 
  which are carried out at a specific stage of the release process.
  
  For example in RichFaces\footnote{RichFaces is a component based library for Java Server Faces, owned and developed by Red Hat} Quality Engineering 
  team\footnote{Quality Engineering team is among the other things responsible for assuring a quality of a product} visual testing was done manually, before releasing 
  the particular version of RichFaces library to a community. In practice it involves building all example applications with new RichFaces libraries, and to go 
  through its use cases with a particular set of web browsers. 
  
  To be more specific, consider please a web page with a chart elements showing a sector composition of gross domestic product in the USA (as figure \ref{fig:richfaces_chart} demonstrates).
  To verify its visual state is not broken, would involve e.g.:
  \begin{enumerate}
   \item Checking the size, overflowing and transparency of all elements in charts.
   \item Checking colors, margins between bars.
   \item Putting a mouse cursor over a specific places in the chart, and verifying whether a popup with more detailed info is rendered in a correct place.
   \item Repeat this for all major browsers\footnote{\label{footnote:majorBrowsers}Major browsers in the time of writing of this thesis are according to the [4]: Google Chrome, Mozilla Firefox, Internet Explorer, Safari, Opera}, 
   and with all supported application containers\footnote{Application containers are special programs 
   dedicated to provide a runtime environment for complex enterprise web applications, e.g. JBoss AS, Wildfly, Apache Tomcat}.
  \end{enumerate}
  
  \begin{figure*}[!htb]
    \begin{center}
    \leavevmode
    \centerline{\scalebox{1.0}{\includegraphics[width=0.9\textwidth]{figures/RichFacesShowcaseChartComponent.png}}}
    \end{center}
    \caption{RichFaces chart component shown in Showcase application}
    \label{fig:richfaces_chart} 
  \end{figure*}
  
  \section{Need for automation}
  The chapter \ref{sec:visual-testing-in-release-process} tried to outline how tedious and error prone might manual visual testing be. From our experience in the RichFaces QE team, any activity
  which needs to be repeated, and does not challenge tester's intellect enough, become a mundane activity. The more one repeats the mundane activity, the more likely an mistake is introduced:
  one forgets to try some use cases of an application, overlooks some minor errors, etc.
  
  Automated visual testing addresses this shortcomings, as it would unburden human resources from mundane activities such as manual testing, and would allow spending their
  time on intellectually more demanding problems. However, it introduces another kind of challenges, and needs to be implemented wisely. Following are minimal requirements 
  for a successful deployment of an automated visual testing.
      
  \section{Requirements for automation}
  \label{sec:requirementsForAutomation}
  An overall cost of the automation has to be taken into consideration. It is necessary to take into account higher initial cost of automation, and consequences it brings: 
  such as increased time to process relatively huge results of testing, cost of test suite maintenance.
  
  Therefore, to foster an effectiveness in quality assurance teams, while keeping the cost of automation reasonable low, automated visual testing would require:
  \begin{enumerate}
   \item A low cost of a test suite maintenance;
   \item a low percentage of false negative or false positive tests results;
   \item a reasonable time to execute the test suite;
   \item a concise yet useful test suite output;
   \item a support for Continuous Integration systems\footnote{Continuous Integration (CI) system is software to facilitate a practice of CI, which in short is about merging 
	  all developer copies with a shared mainline several times a day [5].}.
  \end{enumerate}
  
    \subsection{Low cost of test suite maintenance}
    A test suite needs to reflect a development of an application under test. Therefore, with each change in the application, it is usual that the test suite has to be changed as well.
    Making a change in the test suite can often introduce a bug, and cause false negative or false positive tests results.
    
    To keep this cost as low as possible, the test suite script has to be readable and meaningful, so when the change is about to be introduced, it is clear where and how it should be done.
    
    A test framework in which the test suite is developed should provide high enough abstraction. That would enable better re-usability for various parts of the test suite, 
    while lowering the overall cost of maintenance.
    
    Specifically for visual testing, when done by comparing screen captures, it is very important how well a repository of screen captures is maintainable. Secondly, 
    how reference (those correct ones, other screen captures will be compared with) screen captures are made.
    
    \subsection{Low percentage of false negative or false positive results}
    False negative test results incorrectly indicate a bug in an application under test, while it is a bug in the test suite itself. They are unwanted phenomenon as they take time to process
    and assess correctly.
    
    False positive tests results hide actual bugs in an application. They provide an incorrect feedback by showing the tests as passing, even when there is a bug in the application.
    
    Specifically for visual testing, when it is done by comparison of screen captures, it is very easily to be broken by small changes on a page. For example if the page outputs a current
    date, then it would break with every different date. There has to exist techniques, which would prevent these situations.
    
    \subsection{Reasonable time to execute a test suite}
    Reasonable time is quite subjective matter, but in general, it depends on how many times e.g. per day one needs to run whole test suite. Nowadays trend is a Continuous Integration,
    when a developer or a tester commits changes of an application several times per day to a shared source code mainline. Each such commit should trigger the test suite, which verifies 
    whether the change did not introduced an error to the application.
    
    According to creators of Continuous Integration practice, the whole point of CI is to provide a rapid feedback. A reasonable time for them is 10 minutes. If the build
    takes more time, every minute less is a huge improvement (considering a developer/tester runs test suite several times a day).
    
    \subsection{Concise yet useful test suite output}
    One of drawbacks of automated testing is its ability to produce huge amount of logs, test results etc. The output therefore needs to be as concise as possible, while still providing
    an useful information. A tester needs to be able to quickly recognize where the issue might be. The best situation would be if the tester does not need to run the test again in order
    to spot the issue. The output should give him a clear message where the issue is.
    
    For visual testing specifically, this can be achieved by providing a tester with screen captures together with difference of old and new version.
    
    \subsection{Support for Continuous Integration systems}
    This is quite easily to be achieved, but still, a developer of a tool for visual testing should have this in mind beforehand. Todays CI systems support variety of build systems, for
    various platforms, and languages. For example Jenkins supports build systems like Maven or Gradle, but it can run also shell scripts.
    
    
\chapter{Analysis of existing solutions}
As we introduced in \ref{sec:requirementsForAutomation}, there are many aspects which need to be taken into consideration when automating visual
testing. Following analysis is going to compare existing solutions to automated testing with those requirements in mind, while introducing
different approaches to visual testing.

The representative list of tools for comparison was made also according to an ability to be used in enterprise context. In an enterprise company, there
is a stress on stability and reliability of employed solutions. It is quite vague requirement, and it is usually hard to find out which tools
are a good fit for enterprise companies, however, some indicators, which we used as well, might be helpful:
\begin{itemize}
 \item Is a project actively developed? When was the last release of the project, or how old is the last commit to a source code mainline?
 \item How many opened issues the project has? When was the last activity \\* with those issues ?
 \item What is the quality of the source code? Is it well structured? Does it employ best development practices?
 \item Does the project have a user forum? How active are the users?
 \item Is a big enterprise company behind the solution, or otherwise sponsoring it ?
 \item What are the references if the project is commercialized ?
\end{itemize}

For each tool in following sections we are going to show an example usage and its main drawbacks together with some basic description.
  
  \section{Mogo}
  Mogo [6] approach to visual testing can be in short described like: 
  \begin{enumerate}
   \item One provides set of URLs of an application under test to a cloud based system.
   \item Application URLs are loaded in various browsers, detection of broken links is done.
   \item Screenshots are made and are compared with older screenshots from the same URL to avoid CSS regressions.
  \end{enumerate}
  
  There is no programming script required, therefore it can be used by less skilled human resources. It can be configured in shorter time, and thus is less expensive.
  
  \subsection{Mogo drawbacks}
  
  Drawbacks of this approach are evident when testing dynamic pages, which content is changed easily. Applications which provide rich interaction options to an end user, and which state
  changes by interacting with various web components (calendar widget, table with sorting mechanism etc.), require more subtle way of configuring what actions need to be done before the
  testing itself. Mogo is suitable for testing static web applications, not modern AJAX enabled applications full of CSS transitions.
  
  Above mentioned drawbacks might lead to a bigger number of false negative test results when used with real data (any change, e.g. showing actual date may break testing), or to a bigger 
  number of false positive tests results when such a tool is used to test mocked data \footnote{Mocked data is made up data for purpose of testing, so it is consistent and does not 
  change over time}.
  
  \section{BBC Wraith}
  Wraith is a screenshot comparison tool, created by developers at BBC News [7]. Their approach to visual testing can be described like:
  \begin{enumerate}
   \item Take screenshots of two versions of web application by scripting either PhantomJS \ref{subsec:phantomJS}, or SlimerJS\footnote{SlimerJS is 
   very similar to PhantomJS \ref{subsec:phantomJS}, it just runs on top of Gecko engine, which e.g. Mozilla Firefox runs on top of. [10]} by another JavaScript framework called 
   CasperJS \ref{subsec:casperJS} [20].
   \item One version is the one currently developed (which run on localhost\footnote{In computer networking, \texttt{localhost} means this computer. [11]}), and the other one is a live site.
   \item Once screenshots of web page from these two different environments are captured a command line tool \texttt{imagemagic} is executed to compare screenshots.
   \item Any difference is marked with blue color in a created picture, which is the result of comparing two pictures (It can be seen at Figure \ref{fig:bbcWraithDiff}).
   \item All pictures can be seen in a gallery, which is a generated HTML site (It can be seen at Figure \ref{fig:bbcGallery}).
  \end{enumerate}
  
  To instruct BBC Wraith tool to take screenshots from the web application, one has to firstly script PhantomJS or SlimerJS to interact with the page, and secondly, creates a 
  configuration file, which will tell the PhantomJS instance which URLs need to be loaded and tested. PhantomJS script is one source of distrust to this tool, and therefore is
  introduced furthermore.
  
    \subsection{PhantomJS}
    \label{subsec:phantomJS}
    PhantomJS [8] is stack of web technologies based on headless\footnote{Headless software do not require graphical environment (such as X Windows system) for its execution.} 
    WebKit\footnote{WebKit is a layout engine software component for rendering web pages in web browsers, such as Apple's Safari or previously a Google Chrome [9]} engine, which can be 
    interacted with by using of its JavaScript API.
    
    For the sake of simplicity we can say that it is a browser which does not make any graphical output, which makes testing with such a engine a bit faster and less computer resources 
    demanding.
    
    One can instruct PhantomJS to take a screenshot of a web page with following script:
  
    \begin{verbatim}
      var page = require('webpage').create();
      page.open('http://google.com/', function(status) {
        if(status === 'success') {
          window.setTimeout(function() {
            console.log('Taking screenshot');
            page.render('google.png');
            phantom.exit();
          }, 3000);
        } else {
          console.log('Error with page ');
          phantom.exit();
        }
      });
    \end{verbatim}
    
    When executing such a script it will effectively load \texttt{http://google.com/} web page, waits 3000 milliseconds, and after that, creates a screenshot to the file \texttt{google.png}.
    
    In most environments it will be sufficient to wait those 3000 milliseconds in order to have the \texttt{www.google.com} fully loaded. However, in some resource limited environments, 
    such as virtual machines\footnote{Virtual machines are created to act as real computer systems, run on top of a real hardware}, it does not have to be enough. 
    It will result in massive number of false negative tests. There is a need for more deterministic way of finding out whether the page was loaded fully in given time, and 
    taking of the screenshots itself can take place.
    
    Another problem we noted in the previous script, is its readability. It is written in a too low level way (one has to control HTTP status when loading a page). Secondly, there is a need
    to explicitly call \texttt{page.render('google.png');} in order to take a screenshot. Script which would test a complex web application would be full of such calls. Together with poor
    way of naming created screenshots (a user has to choose name wisely), it might lead to problems when maintaining such a test suite.
    
    PhantomJS API is wrapped by CasperJS, which is furthermore described below.
    
    \subsection{CasperJS}
    \label{subsec:casperJS}
    CasperJS is navigation scripting and testing utility written in JavaScript for the PhantomJS and SlimerJS headless browsers. It eases the process of defining a full navigation scenario 
    and provides useful high-level functions for doing common tasks [20].
    
    Following code snippet shows a simple navigation on Google search web page. It will load \textit{http://google.com} in a browser session, 
    will type into the query input string \textit{MUNI}, and will submit it.
    
    \begin{verbatim}
    casper.start('http://google.com/', function() {
      // search for 'MUNI' from google form
      this.fill('form[action="/search"]', { q: 'MUNI' }, true);
    });
   
    casper.run(function() {
      this.exit();
    });
    \end{verbatim}
    
    The problem with this script, which we identified, is its low-level abstraction of the browser interactions. It makes tests less readable, and thus more error prone when a change
    is needed to be introduced.
    
    \begin{figure*}[!htb]
    \begin{center}
    \leavevmode
    \centerline{\scalebox{1.0}{\includegraphics[width=0.4\textwidth]{figures/bbcWraith.jpg}}}
    \end{center}
    \caption{BBC Wraith picture showing difference in comparison of two web page versions [12]}
    \label{fig:bbcWraithDiff} 
  \end{figure*}

  
  \begin{figure*}[!htb]
    \begin{center}
    \leavevmode
    \centerline{\scalebox{1.0}{\includegraphics[width=0.8\textwidth]{figures/wraithGalleryExample.png}}}
    \end{center}
    \caption{BBC Wraith gallery example [13]}
    \label{fig:bbcGallery} 
  \end{figure*}

    \subsection{BBC Wraith drawbacks}
    Two of the drawback were described in the previous sections, \ref{subsec:phantomJS} and \ref{subsec:casperJS}.
    
    Another problem which might occur when testing with BBC Wraith, is cross browser compatibility. As it supports only PhantomJS, and therefore, one can not assure that the page will be 
    looking the same in all major browsers. The incompatibility comes from the fact, that browsers interpret CSS rules differently, and because they have different JavaScript engines. 
    Thus for example web page might look differently in Google Chrome and Microsoft Internet Explorer, and PhantomJS will not catch this issues.

  \section{Facebook Huxley}
  Another visual testing tool [15], supported by a big company Facebook, Inc. [14], uses similar approach in terms of comparing taken screenshots. The process of taking them, and the process
  of reviewing results is different though.
  
  \begin{enumerate}
   \item One creates a script which would instruct Huxley tool, what web pages screenshots should be taken from. Such a script might look like:
  
    \begin{verbatim}
      [test-page1]
      url=http://localhost:8000/page1.html

      [test-page2]
      url=http://localhost:8000/page2.html
   
      [test-page3]
      url=http://localhost:8000/page3.html
    \end{verbatim}
    
    \item One runs Huxley in the Record mode. That is the mode when Huxley loads the pages automatically in a fresh browser session, and a tester by hitting Enter keyboard button instructs 
    Huxley to take a screenshot. Screenshots are stored in a folder with test name (one given in the square brackets in the example above), together with a 
    JSON\footnote{JSON stands for JavaScript Object Notation, a standard format that uses human readable format to transmit data objects [16]} file describing mainly how long should 
    Huxley wait, when doing visual comparison, to have a tested web page fully loaded. Time is measured during this record mode.
    
    \item To start visual testing, one has to run Huxley in the Playback mode. Huxley will start fresh browser session, and will playback loading of the pages, with waiting for the pages
    to be fully loaded.
    
    \item When there is a change in an application, Huxley will log a warning, and takes a new screenshot automatically. In continuous integration environments, one can instruct Huxley
    to finish with error, in case screenshots are different. In that case, one can run Huxley again with an option to take new screenshots (if the change is desired, and is not an error).
   \end{enumerate}
    
   Main drawback of Facebook Huxley we can see, is similar to BBC Wraith, and that is its non deterministic approach to waiting for a fully loaded web page. It is again a fixed amount of time,
   which can be different from environment to environment. The time to wait can be configured, however, it is still quite error prone, as for first visual testing run
   e.g 4 seconds can be would be enough, and for another run would not.
    
   Secondly it lacks a proper way of viewing results of comparisons, leaving with only one option to check the command line output, together with manual opening of the screenshots. This would degrade
   cooperation among various QA team members, and it is harder to deploy in a software as a service cloud solutions\footnote{Software as a service is on demand software, centrally hosted, 
   accessed typically by using a thin client via web browser [17].}, where such a cooperation might take a place.

  \section{Rusheye}
  \label{sec:rusheye}
  Rusheye [18] is a command line tool, which is focused on automated bulk or on-the-fly inspecting of browser screenshots and comparison with predefined image suite. It enables automated
  visual testing when used together with Selenium 1 project [19].
  
  The process has subtle differences in comparison with previous solutions. It consists from these steps:
  
  \begin{enumerate}
   \item Screenshots are generated, for example by Selenium 1 tool, while functional tests of web application are executed.
   \item First screenshots are claimed to be correct (are controlled manually), they are called patterns.
   \item After a change in web application under test, another run of Selenium 1 tests generates new screenshots. They are called samples.
   \item Patterns and samples are compared, its visual difference is created, and result is saved in an XML file.
   \item The results can be viewed in a desktop application, Rusheye manager [22].
  \end{enumerate}
  
  Rusheye has one very important feature, which another tools lack. It is a possibility to add masks on particular parts of the screenshots. Those parts are ignored when two screenshots are
  compared. It is a huge improvement to protect from false negative tests, as some all the time changing parts (such as actual date, etc.) can be masked from comparison, and thus their change
  will not break testing.
  
  \subsection{Rusheye drawbacks}
  Core of the Rusheye is only able to compare screenshots generated by some other tool. Integration with Selenium 1 is advised, however, functional tests written in Selenium 1 suffer
  from the same problems [21] as scripts written for BBC Wraith \ref{subsec:casperJS}. And that is bad readability caused by their low level coupling with HTML and lack of higher abstraction.
  
  Another problem we can see is only a desktop version of the tool for viewing results (Rusheye Manager). Cooperation on some test suite among QE team members and developers would be 
  more difficult. As they would need to solve persistence of patterns, samples and descriptor of the test suite.
  
  \section{Conclusion of analysis}
  \label{chap:conclusion}
  All previously listed tools have some useful features, which we would like to be inspired with. However, all of the solutions lack something, what we suppose to be an inevitable part of 
  an effective automated visual testing.
  
  Figure \ref{fig:existingSolutionComparison} summarizes requirements we have for a visual testing tool, and the fact whether the tool satisfies the particular requirement.
  
  Tests readability is a problem we discussed with a particular tool description. It is a level of abstraction over underlaying HTML, in which the application is written. It is quite subjective
  matter, however, there are some clues by which it can be made more objective. For example the way how tests are coupled with the HTML or CSS code. Because the more they are, the less they 
  are readable [21]. A scale we used for evaluation supposes insufficient as lowest readability, which in long term run of the test suite might cause lot of issues.
  
  By tests robustness we suppose a level of stability running of the tests with particular tool has. 
  It means how likely there are false positive and false negative tests, whether are caused by not fully loaded page, or by dynamic data rendered on the page. If the robustness is low,
  you can find a red mark in a particular cell. Green one otherwise.
  
  Cross browser compatibility issue deals with ability to test web application across all major browsers \ref{footnote:majorBrowsers}.
  
  By cloud ready features we suppose whether tool has web based system for reviewing results, and thus enables cooperation across QA team members and developers of the particular software.
  
  Continuous Integration friendliness in this context means the fact whether tool is suitable for usage in such systems. It actually means whether output of the tool is clear enough,
  how much work a tester would be required to do manually to deploy such a tool in a test suite. Whether testers would need to review just logs to find visual testing failure, or
  whether it will be somehow visible, e.g. whole test would fail.
  
  \begin{figure*}[!htb]
    \begin{center}
    \leavevmode
    \centerline{\scalebox{1.0}{\includegraphics[width=0.8\textwidth]{figures/existingSolutionsTableComparison.png}}}
    \end{center}
    \caption{Existing solutions features comparison}
    \label{fig:existingSolutionComparison} 
  \end{figure*}
  
  As Figure \ref{fig:existingSolutionComparison} shows, none of the tools met our requirements fully. Therefore, we decided to proceed with developing of a new tool, which would address
  all issues, and which would integrate existing parts of the solutions when it is reasonable. Creation of a new process which would enable effective usage of such
  tool by QA team members is inevitable. Following chapters describe this new tool and the new process.
  
\chapter{New approach}
  Each tool by its definition introduces a process for a visual testing. While we recognized shortcomings (described in the chapter \ref{chap:conclusion}), we realized a need for a different 
  approach to the visual testing. The approach came from 2 and half years of developing and maintaining the functional test suite \footnote{The RichFaces test suite is available at 
  https://github.com/richfaces/richfaces-qa} for RichFaces project \footnote{RichFaces is component library for Java Server Faces web framework [23]}.
  
  \section{Hypothesis}
  It should be enough to have just a functional test suite for a end to end testing of an application. Scripts from functional testing interacts with the application sufficiently, 
  therefore, another script which during such interactions take screenshots is redundant.
  
  This redundancy is expensive, because quality assurance teams need to maintain more test suites. A change in the application needs to be introduced on more places, which allows errors to
  sneak in test suites.
  
  In the same time we do believe, that script for functional testing should be abstracted from a visual testing as much as possible. Meaning that, explicit calls to methods which 
  take screenshots, should be optional. Functional test script should take screenshots automatically during testing, in each important state of an application. By this, we will achieve better
  readability of the functional tests' scripts.
  
  For sure there should be an option to make screenshots explicitly, however, a need to facilitate such option should be sporadic. This will be achieved by fine-grained configuration options.
  A tester should be able to to configure on global level, meaning for all tests from one place, as well as on test level, in what situations a screenshot should be taken.
  
  The base of screenshots which will serve as a reference for a correct state of the web application, will be made automatically during first run of the created tool. 
  Screenshots should be made available automatically for all interested parties (developers, testers, etc.).
  
  A viable solution seems to be introducing a web interface, as a system for managing results of visual testing comparison. In this system (called a manager in this thesis), a user should be
  able to decide about results of visual testing. More particularly asses the results, whether there is a bug in application or not. 
  This web interface will foster cooperation between interested parties.
  
  By following above mentioned principles, we will achieve a better effectiveness of a quality assurance team. More particularly we will massively decrease amount of time they need to spend 
  with manual testing.
  
  \section{Process}
  The whole picture of visual testing would need to include reaction on these problems:
  \begin{enumerate}
   \item Executing a visual test suite for the first time to generate patterns, which new screenshots in subsequent tests executions will be compared with.
   \item Executing the visual test suite second and more times, to generate new screenshots, which are called samples in this thesis, for comparison with patterns generated in the fist run.
   \item Executing the visual test suite when a new test is added, deleted, or otherwise changed.
  \end{enumerate}

  Therefore, overall process can be described with following subprocesses.
  
  \subsection{First execution of functional tests}
  
  Figure \ref{fig:FirstTestsRunBMPN} denotes steps needed to generate patterns. It is a prerequisite for the visual testing.
  
  Screenshots are generated during execution of the tests. If all functional tests pass, those screenshots
  can be claimed as patterns, and are uploaded to a storage. 
  
  If there are errors in functional tests, it can be presumed that screenshots are not correct as well, therefore, one should
  firstly fix failed tests (or mark them to be ignored during testing), and then rerun the tests.
  
  An optional part is reviewing of the screenshots. If there is any issue with patterns, they should be thrown away, and tests will be rerun.
  
  If patterns are correct, a tester can proceed with subsequent runs of the test suite to start the actual visual testing.
  
  \subsection{Subsequent execution of visual testing}
  
  Figure \ref{fig:NextTestsRunsBMPN} shows how subsequent execution of visual testing together with functional testing would look like. The first step is same as in previous process,
  functional tests are executed, and screenshots are taken. Secondly, patterns are downloaded from the storage, and after that actual visual testing can start.
  
  Newly created screenshots, called in this thesis samples, are pixel to pixel compared with downloaded patterns. Result is stored in a descriptor file, and differences between particular
  pattern and sample are visually stored in a generated picture.
  
  If there are found some differences, generated pictures together with their corresponding samples, and patterns are uploaded to a remote storage.
  
  Users should be able to review the results from now on, where he will find the particular run according to a time stamp when the run was executed. They should be able to asses the results,
  with displayed triplet, consisting of patter, sample and their difference. They should be able to say whether it is false negative test result, in which case they should be able to 
  take an action to prevent such results in the future. One of such actions can be applying masks, which is more described in chapter \ref{sec:rusheye}.
  
  The tool should be configurable, so such false negative results are rather sporadic, instead, failed visual comparison tests should reveal a bug in the application under test. In that case
  it is in user responsibility to file such a bug in a corresponding issue tracker. Generated pattern, sample and difference can be used as a useful attachment to a issue report, which
  would better describe actual and expected result.
  
   \begin{figure*}[!htb]
    \begin{center}
    \leavevmode
    \centerline{\scalebox{1.0}{\includegraphics[width=1.0\textwidth]{figures/FirstTestsRunBMPN.png}}}
    \end{center}
    \caption{Process to generate patterns during first execution of functional tests.}
    \label{fig:FirstTestsRunBMPN}
  \end{figure*}
  
  \begin{figure*}[!htb]
    \begin{center}
    \leavevmode
    \centerline{\scalebox{1.0}{\includegraphics[width=1.0\textwidth]{figures/NextTestsRunsBMPN.png}}}
    \end{center}
    \caption{Subsequent execution of visual testing together with functional tests.}
    \label{fig:NextTestsRunsBMPN}
  \end{figure*}
  
  \newpage
  \section{Analysis of useful tool output}
  Requirements for useful output of such a tool based on questionnaire for RichFaces team, or maybe I will ask all JBoss employees
  
\chapter{Implemented tool}
An answer to the new process, requirements: CI viable, reusing what can be reused, extensible, cloud ready, multiple users

  \section{Client part}
  
    \subsection{Arquillian}
    Integration testing, starting containers, event based machine
  
    \subsection{Arquillian Graphene}
    Functional testing of Web UI, screenshooter
  
    \subsection{Rusheye}
    Screenshots comparison, rewritten to Arquillian core
  
    \subsection{Graphene visual testing}
    An adaptor between Rusheye and Arquillian Graphene
  
  \section{Server part}
  
    \subsection{Web application to view results}
    Its architecture, reasoning for chosen solutions, screenshots of app, key functionality
    
    \subsection{Storage of patterns}
    Description of solution, reasoning
    
\chapter{Deployment of tool and process}

  \section{Deployment on production application}
  Deployment on stable app
  
  \section{Deployment on development application}
  Deployment sooner on application which is in Alpha phase, my hypothesis is that it will not be worth to deploy it on such a app, due to too many changes
  
  \section{Usage with CI}
  Jenkins job and its cooperation with the tool, more particullary tool ability to handle multiple jobs, apps, versions, etc.
  
  \section{Cloud ready}
  The app can be easily deployed on Openshift
  
  \section{Results}
  The percentage of improvement of QA effectiveness
  
\chapter{Conclusion}
What I developed, What I improved, What can be better, Possible ways of extensions: Openshift cartridge
    
    % bibtex here
    \addcontentsline{toc}{chapter}{Bibliography}
    \pagestyle{plain}
    \bibliography{thesis}
    %[1] Hetzel, William C., The Complete Guide to Software Testing, 2nd ed. Publication info: Wellesley, Mass. : QED Information Sciences, 1988. ISBN: 0894352423.Physical description: ix, 280 p. : ill ; 24 cm.
    %[2] http://en.wikipedia.org/wiki/Waterfall_model
    %[3] http://en.wikipedia.org/wiki/Scrum_(software_development)
    %[4] http://www.w3schools.com/browsers/browsers_stats.asp
    %[5] http://en.wikipedia.org/wiki/Continuous_integration
    %[6] http://mogotest.com/
    %[7] https://github.com/BBC-News/wraith 
    %[8] http://phantomjs.org/
    %[9] http://en.wikipedia.org/wiki/WebKit
    %[10] http://slimerjs.org/
    %[11] http://en.wikipedia.org/wiki/Localhost
    %[12] http://media.tumblr.com/c5bf7caefde043ee7532b641c7f0e157/tumblr_inline_moaf774b5i1qz4rgp.jpg
    %[13] http://hop.ie/images/posts/wraith/example.png
    %[14] http://en.wikipedia.org/wiki/Facebook
    %[15] https://github.com/facebook/huxley
    %[16] http://en.wikipedia.org/wiki/JSON
    %[17] http://en.wikipedia.org/wiki/Software_as_a_service
    %[18] https://github.com/arquillian/arquillian-rusheye
    %[19] http://www.seleniumhq.org/docs/05_selenium_rc.jsp
    %[20] http://casperjs.org/
    %[21] https://is.muni.cz/auth/th/359345/fi_b/
    %[22] https://github.com/jjamrich/arquillian-rusheye
    %[23] http://richfaces.jboss.org/

\end{document}